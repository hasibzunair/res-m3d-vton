{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5dcb4d0-ec06-4bed-947f-fb6c33523caf",
   "metadata": {},
   "source": [
    "## Demo notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f79ffb-2c50-451e-a2d1-6c0cfa222af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85146e-7c1c-43bf-9ddb-2d37d0679953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "sys.path.insert(0,\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a717ec67-30fb-4232-8531-213dcaa17903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc47f58-9543-47be-a439-0e005078ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c830d-13d3-42ce-aba9-3a1d61a4cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(path):\n",
    "    c = Image.open(path)\n",
    "    c = c.resize((320, 512), Image.BICUBIC).convert('RGB')\n",
    "    c.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb1b95-11d4-42b9-b8d6-d47162199a63",
   "metadata": {},
   "source": [
    "## Resize cloth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cb384-a673-49d4-b468-11fd5bac25e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"../custom_images/cloth2.jpg\"\n",
    "resize_img(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb81f9-d230-46cd-8207-e99c6104fd78",
   "metadata": {},
   "source": [
    "## Resize person image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7983dd-a2a3-4936-9db4-64ebe7239e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"../custom_images/person.png\"\n",
    "resize_img(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e50bd-d32e-4894-9d12-d2a61513151b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc9ff4af-2a23-455f-a427-bc55e3fef280",
   "metadata": {},
   "source": [
    "## Get masks of clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3844d2-1116-45e8-a1c4-55d217b9c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd U-2-Net\n",
    "import u2net_load\n",
    "import u2net_run\n",
    "u2net = u2net_load.model(model_name = 'u2netp')\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69ac4e-daa0-427d-a3c6-bcd9b053b15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc8e68-67f4-452e-88e7-7c594b88aee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aba001c-632a-4e8e-884f-ee19aa7c2d74",
   "metadata": {},
   "source": [
    "Put cloth images in cloth folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11eb15-a254-49c7-8ff5-fe34de2d7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the mask of C \n",
    "u2net_run.infer(u2net, '../mpv3d_example_custom/cloth/', '../mpv3d_example_custom/cloth-mask/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0b8eb-1819-40ae-bf99-b311ca360ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa1513aa-e8d6-4e70-a144-e351e0c7360f",
   "metadata": {},
   "source": [
    "## Get 2D pose of person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83615d-3007-4481-b207-d1b6d3be6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/body_25/pose_iter_584000.caffemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4ca1c-3fff-4af4-b9bf-c7efc22aaae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "class general_pose_model(object):\n",
    "    def __init__(self, modelpath):\n",
    "        # Specify the model to be used\n",
    "        #   Body25: 25 points\n",
    "        #   COCO:   18 points\n",
    "        #   MPI:    15 points\n",
    "        self.inWidth = 368\n",
    "        self.inHeight = 368\n",
    "        self.threshold = 0.05\n",
    "        self.pose_net = self.general_body25_model(modelpath)\n",
    "        \n",
    "    def general_body25_model (self, modelpath):\n",
    "        self.num_points = 25\n",
    "        self.point_pairs = [[1, 0], [1, 2], [1, 5], \n",
    "                            [2, 3], [3, 4], [5, 6], \n",
    "                            [6, 7], [0, 15], [15, 17], \n",
    "                            [0, 16], [16, 18], [1, 8],\n",
    "                            [8, 9], [9, 10], [10, 11], \n",
    "                            [11, 22], [22, 23], [11, 24],\n",
    "                            [8, 12], [12, 13], [13, 14], \n",
    "                            [14, 19], [19, 20], [14, 21]]\n",
    "        prototxt = os.path.join (\n",
    "            modelpath, \n",
    "            \"pose_deploy.prototxt\")\n",
    "        caffemodel = os.path.join (\n",
    "            modelpath, \n",
    "            \"pose_iter_584000.caffemodel\")\n",
    "        coco_model = cv2.dnn.readNetFromCaffe (prototxt, caffemodel)\n",
    "\n",
    "        return coco_model\n",
    "\n",
    "    def predict(self, imgfile):\n",
    "        img_cv2 = cv2.imread(imgfile)\n",
    "        img_height, img_width, _ = img_cv2.shape\n",
    "        inpBlob = cv2.dnn.blobFromImage(img_cv2, \n",
    "                                        1.0 / 255, \n",
    "                                        (self.inWidth, self.inHeight),\n",
    "                                        (0, 0, 0), \n",
    "                                        swapRB=False, \n",
    "                                        crop=False)\n",
    "        self.pose_net.setInput(inpBlob)\n",
    "        self.pose_net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "        self.pose_net.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL)\n",
    "\n",
    "        output = self.pose_net.forward()\n",
    "\n",
    "        H = output.shape[2]\n",
    "        W = output.shape[3]\n",
    "        \n",
    "        points = []\n",
    "        for idx in range(self.num_points):\n",
    "            probMap = output[0, idx, :, :] # confidence map.\n",
    "\n",
    "            # Find global maxima of the probMap.\n",
    "            minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "            # Scale the point to fit on the original image\n",
    "            x = (img_width * point[0]) / W\n",
    "            y = (img_height * point[1]) / H\n",
    "\n",
    "            if prob > self.threshold:\n",
    "                points.append(x)\n",
    "                points.append(y)\n",
    "                points.append(prob)\n",
    "            else:\n",
    "                points.append(0)\n",
    "                points.append(0)\n",
    "                points.append(0)\n",
    "\n",
    "        return points\n",
    "\n",
    "def generate_pose_keypoints(img_file, pose_file):\n",
    "\n",
    "    modelpath = 'pose'\n",
    "    pose_model = general_pose_model(modelpath)\n",
    "\n",
    "    res_points = pose_model.predict(img_file)\n",
    "    \n",
    "    pose_data = {\"version\": 1,\n",
    "                 \"people\":  [\n",
    "                                {\"pose_keypoints_2d\": res_points}\n",
    "                            ]\n",
    "                }\n",
    "\n",
    "    pose_keypoints_path = pose_file\n",
    "\n",
    "    json_object = json.dumps(pose_data, indent = 4) \n",
    "  \n",
    "    # Writing to sample.json \n",
    "    with open(pose_keypoints_path, \"w\") as outfile: \n",
    "        outfile.write(json_object) \n",
    "    print('File saved at {}'.format(pose_keypoints_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4216f-a10a-4279-b978-1bb0492f2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://titanwolf.org/Network/Articles/Article?AID=d4b5fc84-c88e-4159-9a72-a63486d93ce2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc131c-fe31-447d-957a-7bfd5044e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_pose_keypoints(\"/home/hz/m3d-vton/mpv3d_example_custom/image/ZX121DA0E-Q11@6=person_whole_front.png\", \"ZX121DA0E-Q11@6=person_whole_front_keypints.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c5b07-e1c7-42e9-9c1c-c55c0ec1e3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21422e7-8cde-4ca2-a586-d4545e77bac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
