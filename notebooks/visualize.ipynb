{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08843315-3609-4c67-b1f3-29922d30ed85",
   "metadata": {},
   "source": [
    "## Visualize outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b78081b-5e2f-4dea-96d7-f804faf93e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2f59d4-2cfb-4a40-9562-3d54f5c87d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c27a4a-2cd5-4e55-ac51-848259cb2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect, random\n",
    "sys.path.insert(0,\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "#rc('font',**{'family':'sans-serif','sans-serif':['Computer Modern Roman']})\n",
    "#rc('text', usetex=True)\n",
    "\n",
    "import matplotlib.colors\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pytorch_ssim\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bdce517-31de-477f-8708-ce4f6e61876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx, **images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    \n",
    "    #norm=plt.Normalize(0,4) # 5 classes including BG\n",
    "    #map_name = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"black\", \"red\",\"yellow\",\"blue\", \"green\"])\n",
    "    \n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        #plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "        \n",
    "    plt.savefig(\"../outs/compare/{}.png\".format(idx), facecolor=\"white\", bbox_inches = 'tight', dpi=600)\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "def make_dataset(dir):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    f = dir.split('/')[-1].split('_')[-1]\n",
    "    #print (dir, f)\n",
    "    dirs= os.listdir(dir)\n",
    "    for img in dirs:\n",
    "\n",
    "        path = os.path.join(dir, img)\n",
    "        #print(path)\n",
    "        images.append(path)\n",
    "    return images\n",
    "\n",
    "def read_image(path):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e6b50d-f2ef-4ec4-9821-50229b7dacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../outs/compare',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5086de8-e9f5-4774-b719-bc8e240e52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to model\n",
    "#algo1 = \"acgpn_5000\"\n",
    "#algo2 = \"resunet_g1\"\n",
    "\n",
    "# path to tryons\n",
    "\n",
    "# Same\n",
    "# cloth = \"../results/aligned/TFM/test_pairs/cloth\"\n",
    "# person = \"../results/aligned/TFM/test_pairs/person\"\n",
    "# tryon1 = \"../results/aligned_unet/TFM/test_pairs/tryon\"\n",
    "# tryon2 = \"../results/aligned/TFM/test_pairs/tryon\"\n",
    "\n",
    "# Random\n",
    "cloth = \"../results/random/aligned/TFM/test_pairs/cloth\"\n",
    "person = \"../results/random/aligned/TFM/test_pairs/person\"\n",
    "tryon1 = \"../results/random/aligned_unet/TFM/test_pairs/tryon\"\n",
    "tryon2 = \"../results/random/aligned/TFM/test_pairs/tryon\"\n",
    "\n",
    "#algo1_path = \"../tryons_random/\" + algo1 + \"/\" + mode\n",
    "#algo2_path = \"../tryons_random/\" + algo2 + \"/\" + mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de04c7a4-dfa8-4821-a293-e97bfd1abe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloths = sorted(make_dataset(cloth))\n",
    "persons = sorted(make_dataset(person))\n",
    "tryon1p = sorted(make_dataset(tryon1))\n",
    "tryon2p = sorted(make_dataset(tryon2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdace0ad-1449-48f2-84dd-00b1c9c1cf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tryon2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59796524-a8bd-4c9d-b855-cf4a1f0d654b",
   "metadata": {},
   "source": [
    "### Visualize different algo output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e188a79-e028-42c3-9f1a-387ff13e0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slc = 934\n",
    "# i = 0\n",
    "# for c, p, a1, a2 in zip(cloths[:slc], persons[:slc], tryon1p[:slc], \n",
    "#                      tryon2p[:slc]):\n",
    "    \n",
    "#     # visualize(i,\n",
    "#     #           unet=read_image(a1), \n",
    "#     #           resunet = read_image(a2))\n",
    "#     visualize(i, \n",
    "#               person=read_image(p),\n",
    "#               cloth=read_image(c),\n",
    "#               unet=read_image(a1), \n",
    "#               resunet = read_image(a2))\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27496f08-b9a9-472b-8545-371e798e1e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e507f0-8321-42dc-8bdb-97ebd313b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nums = np.array([4,5,22,45,87,123,10,52,95,\n",
    "#                  99,323,586,149,188,787,806,829,323,929,325,241,193,276,294,306])\n",
    "\n",
    "# nums = nums[:12]\n",
    "\n",
    "nums = [4,5,22,45,87,123,10,52,95,\n",
    "                 99,323,586,149,188,787,806,829,325,241,306,276,294]\n",
    "\n",
    "nums = [123, 829, 10, 99, 276, 95, 52, 294] #random.sample(nums, 8)\n",
    "\n",
    "rows = int(len(nums) / 2)\n",
    "cols = 8\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(22, 18), constrained_layout=True)\n",
    "\n",
    "v = 0\n",
    "for r in range(rows):\n",
    "    rp1=read_image(persons[nums[v+r]])\n",
    "    tc1=read_image(cloths[nums[v+r]])\n",
    "    a1=read_image(tryon1p[nums[v+r]])\n",
    "    o1=read_image(tryon2p[nums[v+r]])\n",
    "    \n",
    "    rp2=read_image(persons[nums[v+r+1]])\n",
    "    tc2=read_image(cloths[nums[v+r+1]])\n",
    "    a2=read_image(tryon1p[nums[v+r+1]])\n",
    "    o2=read_image(tryon2p[nums[v+r+1]])\n",
    "    \n",
    "    # rp3=read_image(persons[nums[v+r+2]])\n",
    "    # tc3=read_image(cloths[nums[v+r+2]])\n",
    "    # a3=read_image(tryon1p[nums[v+r+2]])\n",
    "    # o3=read_image(tryon2p[nums[v+r+2]])\n",
    "    \n",
    "    v+=1\n",
    "    #images = [rp1, tc1, a1, o1, rp2, tc2, a2, o2, rp3, tc3, a3, o3]\n",
    "    images = [rp1, tc1, a1, o1, rp2, tc2, a2, o2]\n",
    "    \n",
    "    captions = [\"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\", \n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\", \n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\"]\n",
    "    \n",
    "    for c in range(cols):\n",
    "        axarr[r, c].imshow(images[c], cmap='gray')\n",
    "        axarr[r, c].axis(\"off\")\n",
    "        axarr[r, c].set_aspect('equal')\n",
    "        if r==0:\n",
    "            axarr[r, c].set_title(captions[c], fontsize=25)\n",
    "\n",
    "plt.savefig(\"../outs/visualization_tryons_small.pdf\", facecolor=\"white\", bbox_inches = 'tight', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d6467-e9fb-4b34-bde1-e0d8e9e31fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f3e71-f261-4023-9cf3-fd9e585f940d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ee61b1-4036-4d60-83ec-9e360890d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6c556-f740-4859-bf20-1acd564c7a1f",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fb411-a2ed-402b-a65b-019ddaf48346",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.array([4,5,22,45,87,123,10,52,95,\n",
    "                 99,323,586,149,188,787,806,829,193,929,325,241,306,276,294])\n",
    "\n",
    "nums = np.array([4,5,22,45,87,123,10,52,95,\n",
    "                 99,323,149,188,787,806,829,325,241,306,276,294])\n",
    "\n",
    "rows = int(7)\n",
    "cols = 12\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(33, 31), constrained_layout=True)\n",
    "\n",
    "v = 0\n",
    "for r in range(rows):\n",
    "    rp1=read_image(persons[nums[v+r]])\n",
    "    tc1=read_image(cloths[nums[v+r]])\n",
    "    a1=read_image(tryon1p[nums[v+r]])\n",
    "    o1=read_image(tryon2p[nums[v+r]])\n",
    "    \n",
    "    rp2=read_image(persons[nums[v+r+1]])\n",
    "    tc2=read_image(cloths[nums[v+r+1]])\n",
    "    a2=read_image(tryon1p[nums[v+r+1]])\n",
    "    o2=read_image(tryon2p[nums[v+r+1]])\n",
    "    \n",
    "    rp3=read_image(persons[nums[v+r+2]])\n",
    "    tc3=read_image(cloths[nums[v+r+2]])\n",
    "    a3=read_image(tryon1p[nums[v+r+2]])\n",
    "    o3=read_image(tryon2p[nums[v+r+2]])\n",
    "    \n",
    "    v+=2\n",
    "    images = [rp1, tc1, a1, o1, rp2, tc2, a2, o2, rp3, tc3, a3, o3]\n",
    "    \n",
    "    captions = [\"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\", \n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\", \n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\"]\n",
    "    \n",
    "    for c in range(cols):\n",
    "        axarr[r, c].imshow(images[c], cmap='gray')\n",
    "        axarr[r, c].axis(\"off\")\n",
    "        axarr[r, c].set_aspect('equal')\n",
    "        if r==0:\n",
    "            axarr[r, c].set_title(captions[c], fontsize=35)\n",
    "\n",
    "plt.savefig(\"../outs/visualization_tryons_all.pdf\", facecolor=\"white\", bbox_inches = 'tight', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66845b1a-22b5-4903-a95b-46597abeeece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0af8c6-c032-4d11-a273-0559d507d7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74745f5-61fe-4cfd-806f-df5f1229abca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e8053-c716-4195-9a68-8d4a7f1ec125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a42d7-277f-4fdb-99aa-588fb9741d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e003563-1ec7-4a42-8788-aeedec42cca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7355e3f-91c6-4efc-99cf-a553482dec57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd5b56-bdc7-4769-a96c-ced26750c79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "257cdf7c-d159-4747-b894-8f08391c309f",
   "metadata": {},
   "source": [
    "### Compute FID scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33fefe86-0f54-43e6-8722-298f66e9bb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  7.24it/s]\n",
      "100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  8.55it/s]\n",
      "FID:  -2.020402860125614e-05\n"
     ]
    }
   ],
   "source": [
    "!python -m pytorch_fid \"/home/hz/m3d-vton/results/aligned_unet/TFM/test_pairs/person/\" \"/home/hz/m3d-vton/results/aligned_unet/TFM/test_pairs/person/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f2218-27d8-48ef-ad12-31c04a258107",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "398e9ac6-fb07-4d1a-9927-e34e715dd567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  7.33it/s]\n",
      "100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  8.85it/s]\n",
      "FID:  19.875877419374007\n"
     ]
    }
   ],
   "source": [
    "!python -m pytorch_fid \"/home/hz/m3d-vton/results/aligned_unet/TFM/test_pairs/person/\" \"/home/hz/m3d-vton/results/aligned_unet/TFM/test_pairs/tryon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643c953-39ec-4e01-89fa-f1ee23d0d008",
   "metadata": {},
   "source": [
    "#### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f6abac2-9062-42ef-ba0a-f3c70c5be93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  6.95it/s]\n",
      "100%|███████████████████████████████████████████| 19/19 [00:02<00:00,  8.57it/s]\n",
      "FID:  15.162646143627711\n"
     ]
    }
   ],
   "source": [
    "!python -m pytorch_fid \"/home/hz/m3d-vton/results/aligned_unet/TFM/test_pairs/person/\" \"/home/hz/m3d-vton/results/aligned/TFM/test_pairs/tryon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb636b7e-6c32-4109-9fa9-c88ea364bc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67edb2-acc2-4916-8768-443b80c0761d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e913296a-5ca6-4a47-bd5e-fbb9a79fa3d5",
   "metadata": {},
   "source": [
    "### Compute SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dede12ff-bfbd-4998-b7d6-954cfb14d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "gt = \"/home/hz/m3d-vton/results/aligned_unet/TFM/test_pairs/person/\"\n",
    "a1 = \"/home/hz/m3d-vton/results/aligned_unet/TFM/test_pairs/tryon\"\n",
    "a2 = \"/home/hz/m3d-vton/results/aligned/TFM/test_pairs/tryon\"\n",
    "\n",
    "gtp = sorted(make_dataset(gt))\n",
    "a1p = sorted(make_dataset(a1))\n",
    "a2p = sorted(make_dataset(a2))\n",
    "\n",
    "for x,y in zip(gtp, a2p):\n",
    "    img1 = read_image(x) / 255.0\n",
    "    img2 = read_image(y) / 255.0\n",
    "    \n",
    "    img1 = np.expand_dims(img1, axis=0)\n",
    "    img2 = np.expand_dims(img2, axis=0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        img1 = torch.from_numpy(img1)\n",
    "        img2 = torch.from_numpy(img2)\n",
    "        \n",
    "        #print(img1.shape, img2.shape)\n",
    "        score = pytorch_ssim.ssim(img1, img2)\n",
    "        score = score.item()\n",
    "        #print(score)\n",
    "        scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716bad49-0fb8-403c-b8d8-873c53936eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef8868a6-d072-457f-b907-fd39181f793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM score: 0.9814\n"
     ]
    }
   ],
   "source": [
    "score = np.array(scores).mean()\n",
    "print(\"SSIM score: {:.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e05d6-6872-48df-847d-0f0388165f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fa69c-a28b-4d6a-9bf4-105ab44040c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4279ee-95eb-43df-ae21-ede52654cc29",
   "metadata": {},
   "source": [
    "| Method | FID (↓) | SSIM (↑)|\n",
    "|--------|------|-----|\n",
    "| VITON (CVPR, 2018) | 28.43 | 0.8807 |\n",
    "| CP-VTON (ECCV, 2018) | 20.05 | 0.8503 |\n",
    "| CP-VTON+ (CVPRW, 2020) | 23.18 | 0.8782 |\n",
    "| ACGPN (CVPR, 2020) | 20.19 | 0.8924 |\n",
    "| M3D-VTON (ICCV, 2021) | 19.875 | 0.9725 |\n",
    "| Ours | **15.162** | **0.9814** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826bf3e-7d2f-4f2c-9ceb-d1ab7584cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
