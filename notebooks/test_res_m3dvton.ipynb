{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08843315-3609-4c67-b1f3-29922d30ed85",
   "metadata": {},
   "source": [
    "## Test model, visualize outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78081b-5e2f-4dea-96d7-f804faf93e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f59d4-2cfb-4a40-9562-3d54f5c87d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c27a4a-2cd5-4e55-ac51-848259cb2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect, random\n",
    "sys.path.insert(0,\"..\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "#rc('font',**{'family':'sans-serif','sans-serif':['Computer Modern Roman']})\n",
    "#rc('text', usetex=True)\n",
    "\n",
    "import matplotlib.colors\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "\n",
    "import pytorch_ssim\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdce517-31de-477f-8708-ce4f6e61876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx, **images):\n",
    "    \"\"\"Plot images in one row.\"\"\"\n",
    "    \n",
    "    #norm=plt.Normalize(0,4) # 5 classes including BG\n",
    "    #map_name = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"black\", \"red\",\"yellow\",\"blue\", \"green\"])\n",
    "    \n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        #plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "        \n",
    "    plt.savefig(\"../outs/compare/{}.png\".format(idx), facecolor=\"white\", bbox_inches = 'tight', dpi=600)\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "def make_dataset(dir):\n",
    "    images = []\n",
    "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
    "\n",
    "    f = dir.split('/')[-1].split('_')[-1]\n",
    "    #print (dir, f)\n",
    "    dirs= os.listdir(dir)\n",
    "    for img in dirs:\n",
    "\n",
    "        path = os.path.join(dir, img)\n",
    "        #print(path)\n",
    "        images.append(path)\n",
    "    return images\n",
    "\n",
    "def read_image(path):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (320, 512))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e6b50d-f2ef-4ec4-9821-50229b7dacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('../outs/compare',exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5086de8-e9f5-4774-b719-bc8e240e52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to tryons\n",
    "\n",
    "# Same\n",
    "# cloth = \"../results/same/aligned/TFM/test_pairs/cloth\"\n",
    "# person = \"../results/same/aligned/TFM/test_pairs/person\"\n",
    "# tryon1 = \"../results/same/aligned_unet/TFM/test_pairs/tryon\"\n",
    "# tryon2 = \"../results/same/aligned/TFM/test_pairs/tryon\"\n",
    "\n",
    "# Random\n",
    "cloth = \"../results/random/aligned/TFM/test_pairs/cloth\"\n",
    "person = \"../results/random/aligned/TFM/test_pairs/person\"\n",
    "tryon1 = \"../results/random/aligned_unet/TFM/test_pairs/tryon\"\n",
    "tryon2 = \"../results/random/aligned/TFM/test_pairs/tryon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04c7a4-dfa8-4821-a293-e97bfd1abe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloths = sorted(make_dataset(cloth))\n",
    "persons = sorted(make_dataset(person))\n",
    "tryon1p = sorted(make_dataset(tryon1))\n",
    "tryon2p = sorted(make_dataset(tryon2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdace0ad-1449-48f2-84dd-00b1c9c1cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tryon2p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59796524-a8bd-4c9d-b855-cf4a1f0d654b",
   "metadata": {},
   "source": [
    "### Visualize different algo output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e188a79-e028-42c3-9f1a-387ff13e0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slc = 934\n",
    "# i = 0\n",
    "# for c, p, a1, a2 in zip(cloths[:slc], persons[:slc], tryon1p[:slc], \n",
    "#                      tryon2p[:slc]):\n",
    "    \n",
    "#     # visualize(i,\n",
    "#     #           unet=read_image(a1), \n",
    "#     #           resunet = read_image(a2))\n",
    "#     visualize(i, \n",
    "#               person=read_image(p),\n",
    "#               cloth=read_image(c),\n",
    "#               unet=read_image(a1), \n",
    "#               resunet = read_image(a2))\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3dd8af-a5a6-4f16-9f01-de15ba95c9e3",
   "metadata": {},
   "source": [
    "### Vis subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e507f0-8321-42dc-8bdb-97ebd313b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nums = np.array([4,5,22,45,87,123,10,52,95,\n",
    "#                  99,323,586,149,188,787,806,829,323,929,325,241,193,276,294,306])\n",
    "\n",
    "# nums = nums[:12]\n",
    "\n",
    "nums = [4,5,22,45,87,123,10,52,95,\n",
    "                 99,323,586,149,188,787,806,829,325,241,306,276,294]\n",
    "\n",
    "# Plot these\n",
    "nums = [123, 829, 10, 99, 276, 95, 52, 294] #random.sample(nums, 8)\n",
    "\n",
    "rows = int(len(nums) / 2)\n",
    "cols = 8\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(22, 18), constrained_layout=True)\n",
    "\n",
    "v = 0\n",
    "for r in range(rows):\n",
    "    rp1=read_image(persons[nums[v+r]])\n",
    "    tc1=read_image(cloths[nums[v+r]])\n",
    "    a1=read_image(tryon1p[nums[v+r]])\n",
    "    o1=read_image(tryon2p[nums[v+r]])\n",
    "    \n",
    "    rp2=read_image(persons[nums[v+r+1]])\n",
    "    tc2=read_image(cloths[nums[v+r+1]])\n",
    "    a2=read_image(tryon1p[nums[v+r+1]])\n",
    "    o2=read_image(tryon2p[nums[v+r+1]])\n",
    "    \n",
    "    # rp3=read_image(persons[nums[v+r+2]])\n",
    "    # tc3=read_image(cloths[nums[v+r+2]])\n",
    "    # a3=read_image(tryon1p[nums[v+r+2]])\n",
    "    # o3=read_image(tryon2p[nums[v+r+2]])\n",
    "    \n",
    "    v+=1\n",
    "    #images = [rp1, tc1, a1, o1, rp2, tc2, a2, o2, rp3, tc3, a3, o3]\n",
    "    images = [rp1, tc1, a1, o1, rp2, tc2, a2, o2]\n",
    "    \n",
    "    captions = [\"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\", \n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\", \n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\"]\n",
    "    \n",
    "    for c in range(cols):\n",
    "        axarr[r, c].imshow(images[c], cmap='gray')\n",
    "        axarr[r, c].axis(\"off\")\n",
    "        axarr[r, c].set_aspect('equal')\n",
    "        if r==0:\n",
    "            axarr[r, c].set_title(captions[c], fontsize=25)\n",
    "\n",
    "plt.savefig(\"../outs/visualization_tryons_small.pdf\", facecolor=\"white\", bbox_inches = 'tight', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f3e71-f261-4023-9cf3-fd9e585f940d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf6c556-f740-4859-bf20-1acd564c7a1f",
   "metadata": {},
   "source": [
    "### All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fb411-a2ed-402b-a65b-019ddaf48346",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = np.array([4,5,22,45,87,123,10,52,95,\n",
    "                 99,323,586,149,188,787,806,829,193,929,325,241,306,276,294])\n",
    "\n",
    "nums = np.array([4,5,22,45,87,123,10,52,95,\n",
    "                 99,323,149,188,787,806,829,325,241,306,276,294])\n",
    "\n",
    "#nums = np.array([99,323,149,188,787,806,829,325,241,306,276,294])\n",
    "\n",
    "\n",
    "rows = int(7)\n",
    "cols = 12\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(33, 31), constrained_layout=True)\n",
    "#fig, axarr = plt.subplots(rows, cols, figsize=(33, 15), constrained_layout=True)\n",
    "alphabet_string = string.ascii_lowercase\n",
    "alphabet_list = list(alphabet_string)\n",
    "\n",
    "v = 0\n",
    "for r in range(rows):\n",
    "    rp1=read_image(persons[nums[v+r]])\n",
    "    w,h = (1,50)\n",
    "    fs = 2\n",
    "    color = (255,255,255)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX #FONT_HERSHEY_DUPLEX  #press tab for different operations\n",
    "    cv2.putText(rp1, str(alphabet_list[v+r]), (w,h), font, fs, color, 3, cv2.LINE_AA)\n",
    "    \n",
    "    tc1=read_image(cloths[nums[v+r]])\n",
    "    a1=read_image(tryon1p[nums[v+r]])\n",
    "    o1=read_image(tryon2p[nums[v+r]])\n",
    "    \n",
    "    rp2=read_image(persons[nums[v+r+1]])\n",
    "    cv2.putText(rp2, str(alphabet_list[v+r+1]), (w,h), font, fs, color, 3, cv2.LINE_AA)\n",
    "    tc2=read_image(cloths[nums[v+r+1]])\n",
    "    a2=read_image(tryon1p[nums[v+r+1]])\n",
    "    o2=read_image(tryon2p[nums[v+r+1]])\n",
    "    \n",
    "    rp3=read_image(persons[nums[v+r+2]])\n",
    "    cv2.putText(rp3, str(alphabet_list[v+r+2]), (w,h), font, fs, color, 3, cv2.LINE_AA)\n",
    "    tc3=read_image(cloths[nums[v+r+2]])\n",
    "    a3=read_image(tryon1p[nums[v+r+2]])\n",
    "    o3=read_image(tryon2p[nums[v+r+2]])\n",
    "    \n",
    "    v+=2\n",
    "    images = [rp1, tc1, a1, o1, rp2, tc2, a2, o2, rp3, tc3, a3, o3]\n",
    "    \n",
    "    captions = [\"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\", \n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\", \n",
    "                \"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON\", \"Ours\"]\n",
    "    \n",
    "    for c in range(cols):\n",
    "        axarr[r, c].imshow(images[c], cmap='gray')\n",
    "        axarr[r, c].axis(\"off\")\n",
    "        axarr[r, c].set_aspect('equal')\n",
    "        if r==0:\n",
    "            axarr[r, c].set_title(captions[c], fontsize=35)\n",
    "\n",
    "plt.savefig(\"../outs/visualization_tryons_all.png\", facecolor=\"white\", bbox_inches = 'tight', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66845b1a-22b5-4903-a95b-46597abeeece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9229a3-9f01-4c02-b8d5-070831514950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14755f5e-61dc-4d73-b12c-19c9bcb84999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d20b4e64-89eb-4955-827f-0548e67a4160",
   "metadata": {},
   "source": [
    "### Visualize full input, 2D, 3D outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dde6ef-1ac6-423f-9590-625dcee2f8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0af8c6-c032-4d11-a273-0559d507d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nums = np.array([4,5,22,45,87,123,10,52,95,\n",
    "                 99,323,149,188,787,806,829,325,241,306,276,294])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74745f5-61fe-4cfd-806f-df5f1229abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e129b8f-6125-4108-8129-77e7b8b50bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons[nums[4]], persons[nums[8]], persons[nums[9]], persons[nums[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1fbda-ae26-4baf-9a2c-b01564806cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_(path):\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    #image = cv2.resize(image, (320, 512))\n",
    "    image = center_crop(image, (320, 512))\n",
    "    #image = cv2.resize(image, (320, 512))\n",
    "    image = scale_image(image, factor=1)\n",
    "    return image\n",
    "\n",
    "\n",
    "# Code taken from https://gist.github.com/Nannigalaxy/35dd1d0722f29672e68b700bc5d44767\n",
    "def center_crop(img, dim):\n",
    "\t\"\"\"Returns center cropped image\n",
    "\tArgs:\n",
    "\timg: image to be center cropped\n",
    "\tdim: dimensions (width, height) to be cropped\n",
    "\t\"\"\"\n",
    "\twidth, height = img.shape[1], img.shape[0]\n",
    "\n",
    "\t# process crop width and height for max available dimension\n",
    "\tcrop_width = dim[0] if dim[0]<img.shape[1] else img.shape[1]\n",
    "\tcrop_height = dim[1] if dim[1]<img.shape[0] else img.shape[0] \n",
    "\tmid_x, mid_y = int(width/2), int(height/2)\n",
    "\tcw2, ch2 = int(crop_width/2), int(crop_height/2) \n",
    "\tcrop_img = img[mid_y-ch2:mid_y+ch2, mid_x-cw2:mid_x+cw2]\n",
    "\treturn crop_img\n",
    "\n",
    "def scale_image(img, factor=1):\n",
    "\t\"\"\"Returns resize image by scale factor.\n",
    "\tThis helps to retain resolution ratio while resizing.\n",
    "\tArgs:\n",
    "\timg: image to be scaled\n",
    "\tfactor: scale factor to resize\n",
    "\t\"\"\"\n",
    "\treturn cv2.resize(img,(int(img.shape[1]*factor), int(img.shape[0]*factor)))\n",
    "\n",
    "\n",
    "# def image_resize(path, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    \n",
    "#     image = cv2.imread(path)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     # initialize the dimensions of the image to be resized and\n",
    "#     # grab the image size\n",
    "#     dim = None\n",
    "#     (h, w) = image.shape[:2]\n",
    "\n",
    "#     # if both the width and height are None, then return the\n",
    "#     # original image\n",
    "#     if width is None and height is None:\n",
    "#         return image\n",
    "\n",
    "#     # check to see if the width is None\n",
    "#     if width is None:\n",
    "#         # calculate the ratio of the height and construct the\n",
    "#         # dimensions\n",
    "#         r = height / float(h)\n",
    "#         dim = (int(w * r), height)\n",
    "\n",
    "#     # otherwise, the height is None\n",
    "#     else:\n",
    "#         # calculate the ratio of the width and construct the\n",
    "#         # dimensions\n",
    "#         r = width / float(w)\n",
    "#         dim = (width, int(h * r))\n",
    "\n",
    "#     # resize the image\n",
    "#     resized = cv2.resize(image, dim, interpolation = inter)\n",
    "\n",
    "#     # return the resized image\n",
    "#     return resized\n",
    "\n",
    "\n",
    "rp1=read_image(persons[nums[4]])\n",
    "w,h = (1,50)\n",
    "fs = 2\n",
    "color = (255,255,255)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX #FONT_HERSHEY_DUPLEX  #press tab for different operations\n",
    "cv2.putText(rp1, str('a'), (w,h), font, fs, color, 3, cv2.LINE_AA)\n",
    "tc1=read_image(cloths[nums[4]])\n",
    "a1=read_image(tryon1p[nums[4]])\n",
    "o1=read_image(tryon2p[nums[4]])\n",
    "\n",
    "rp2=read_image(persons[nums[8]])\n",
    "cv2.putText(rp2, str('b'), (w,h), font, fs, color, 3, cv2.LINE_AA)\n",
    "tc2=read_image(cloths[nums[8]])\n",
    "a2=read_image(tryon1p[nums[8]])\n",
    "o2=read_image(tryon2p[nums[8]])\n",
    "\n",
    "rp3=read_image(persons[nums[9]])\n",
    "cv2.putText(rp3, str('c'), (w,h), font, fs, color, 3, cv2.LINE_AA)\n",
    "tc3=read_image(cloths[nums[9]])\n",
    "a3=read_image(tryon1p[nums[9]])\n",
    "o3=read_image(tryon2p[nums[9]])\n",
    "\n",
    "rp4=read_image(persons[nums[10]])\n",
    "cv2.putText(rp4, str('d'), (w,h), font, fs, color, 3, cv2.LINE_AA)\n",
    "tc4=read_image(cloths[nums[10]])\n",
    "a4=read_image(tryon1p[nums[10]])\n",
    "o4=read_image(tryon2p[nums[10]])\n",
    "\n",
    "\n",
    "# a13d = image_resize(\"../assets/unet1.png\")\n",
    "# a13dd = image_resize(\"../assets/unet2.png\")\n",
    "\n",
    "pcu1 = read_image_(\"../assets/unet1.png\")\n",
    "pcu2 = read_image_(\"../assets/unet2.png\")\n",
    "pcu3 = read_image_(\"../assets/unet3.png\")\n",
    "pcu4 = read_image_(\"../assets/unet4.png\")\n",
    "pcur1 = read_image_(\"../assets/resunet1.png\")\n",
    "pcur2 = read_image_(\"../assets/resunet2.png\")\n",
    "pcur3 = read_image_(\"../assets/resunet3.png\")\n",
    "pcur4 = read_image_(\"../assets/resunet4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad0b9e-0886-4d27-81be-385e2ea56bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rows = int(4)\n",
    "cols = 6\n",
    "fig, axarr = plt.subplots(rows, cols, figsize=(20, 22), constrained_layout=True)\n",
    "\n",
    "# [rp1, tc1, a1, o1, pcu2, pcur2],\n",
    "# [rp3, tc3, a3, o3, pcu3, pcur3],\n",
    "\n",
    "images = [[rp1, tc1, a1, o1, pcu2, pcur2],\n",
    "          [rp2, tc2, a2, o2, pcu1, pcur1],\n",
    "          [rp3, tc3, a3, o3, pcu3, pcur3],\n",
    "          [rp4, tc4, a4, o4, pcu4, pcur4]]\n",
    "    \n",
    "captions = [\"Reference \\n Person\", \"Target \\n Clothes\", \"M3D-VTON (2D)\", \"Ours (2D)\", \"M3D-VTON \\n (3D Mesh)\", \"Ours \\n (3D Mesh)\",]\n",
    "v=0\n",
    "for r in range(rows):\n",
    "    img = images[r]\n",
    "    for c in range(cols):\n",
    "        axarr[r, c].imshow(img[c], cmap='gray')\n",
    "        axarr[r, c].axis(\"off\")\n",
    "        axarr[r, c].set_aspect('equal')\n",
    "        if r==0:\n",
    "            axarr[r, c].set_title(captions[c], fontsize=25)\n",
    "            \n",
    "    v+=1\n",
    "\n",
    "plt.savefig(\"../outs/vis_3d.png\", facecolor=\"white\", bbox_inches = 'tight', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14724d-a5a4-40e3-9305-eeadd90a2a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb62407-84e2-439a-8ea4-52bcf63bcf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8830c-f23b-4b80-a718-16e53d8b6bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd5b56-bdc7-4769-a96c-ced26750c79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "257cdf7c-d159-4747-b894-8f08391c309f",
   "metadata": {},
   "source": [
    "### Compute FID scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fefe86-0f54-43e6-8722-298f66e9bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytorch_fid \"/home/hz/m3d-vton/results/same/aligned_unet/TFM/test_pairs/person/\" \"/home/hz/m3d-vton/results/same/aligned_unet/TFM/test_pairs/person/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f2218-27d8-48ef-ad12-31c04a258107",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398e9ac6-fb07-4d1a-9927-e34e715dd567",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytorch_fid \"/home/hz/m3d-vton/results/same/aligned_unet/TFM/test_pairs/person/\" \"/home/hz/m3d-vton/results/same/aligned_unet/TFM/test_pairs/tryon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e643c953-39ec-4e01-89fa-f1ee23d0d008",
   "metadata": {},
   "source": [
    "#### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6abac2-9062-42ef-ba0a-f3c70c5be93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pytorch_fid \"/home/hz/m3d-vton/results/same/aligned_unet/TFM/test_pairs/person/\" \"/home/hz/m3d-vton/results/same/aligned/TFM/test_pairs/tryon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb636b7e-6c32-4109-9fa9-c88ea364bc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67edb2-acc2-4916-8768-443b80c0761d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e913296a-5ca6-4a47-bd5e-fbb9a79fa3d5",
   "metadata": {},
   "source": [
    "### Compute SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede12ff-bfbd-4998-b7d6-954cfb14d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "gt = \"/home/hz/m3d-vton/results/same/aligned_unet/TFM/test_pairs/person/\"\n",
    "a1 = \"/home/hz/m3d-vton/results/same/aligned_unet/TFM/test_pairs/tryon\"\n",
    "a2 = \"/home/hz/m3d-vton/results/same/aligned/TFM/test_pairs/tryon\"\n",
    "\n",
    "gtp = sorted(make_dataset(gt))\n",
    "a1p = sorted(make_dataset(a1))\n",
    "a2p = sorted(make_dataset(a2))\n",
    "\n",
    "for x,y in zip(gtp, a2p):\n",
    "    img1 = read_image(x) / 255.0\n",
    "    img2 = read_image(y) / 255.0\n",
    "    \n",
    "    img1 = np.expand_dims(img1, axis=0)\n",
    "    img2 = np.expand_dims(img2, axis=0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        img1 = torch.from_numpy(img1)\n",
    "        img2 = torch.from_numpy(img2)\n",
    "        \n",
    "        #print(img1.shape, img2.shape)\n",
    "        score = pytorch_ssim.ssim(img1, img2)\n",
    "        score = score.item()\n",
    "        #print(score)\n",
    "        scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716bad49-0fb8-403c-b8d8-873c53936eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8868a6-d072-457f-b907-fd39181f793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.array(scores).mean()\n",
    "print(\"SSIM score: {:.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6e05d6-6872-48df-847d-0f0388165f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fa69c-a28b-4d6a-9bf4-105ab44040c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4279ee-95eb-43df-ae21-ede52654cc29",
   "metadata": {},
   "source": [
    "| Method | FID (↓) | SSIM (↑)|\n",
    "|--------|------|-----|\n",
    "| VITON (CVPR, 2018) | 28.43 | 0.8807 |\n",
    "| CP-VTON (ECCV, 2018) | 20.05 | 0.8503 |\n",
    "| CP-VTON+ (CVPRW, 2020) | 23.18 | 0.8782 |\n",
    "| ACGPN (CVPR, 2020) | 20.19 | 0.8924 |\n",
    "| M3D-VTON (ICCV, 2021) | 19.875 | 0.9725 |\n",
    "| Ours | **15.162** | **0.9814** |\n",
    "| Ours | **14.591** | **0.9817** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826bf3e-7d2f-4f2c-9ceb-d1ab7584cedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
